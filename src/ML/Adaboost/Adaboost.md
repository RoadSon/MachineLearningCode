Bagging方法就是基于第二周实现的决策树以及第四周采用的bootstrapping方法，而随机森林就是在bagging 的基础上引入随机属性选择，所以这里不再多说。
Bagging以决策树作为基分类器不能处理离散型数据集，因为随机取样有可能取不到测试集的类别，例如测试集的某个属性有类别3，而训练集中该属性只有类别1，2，在使用训练集构建出决策树后无法处理类别3的数据。
接下来重点介绍Adaboost算法。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191110150306708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTgyODc4NQ==,size_16,color_FFFFFF,t_70)
以单层决策树作为弱分类器，用《机器学习实战》第七章的马疝病数据集来测试我编写的代码，书上的结果如表7-1所示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191110150326867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTgyODc4NQ==,size_16,color_FFFFFF,t_70)
我运行的结果如表7-2所示。
**表7-2 Java实现Adaboost在马疝病数据集上的结果**
|分类器个数|训练集误差|测试集误差|
|----|----|----|
|1|0.28|0.27|
|10|0.21|0.21| 
|50|0.18|0.22| 
|100|0.16|0.24|
|500|0.05|0.33|
|1000|0.02|0.33|
|2000|0.00|0.36|
正如书上所说，观察表7-1中的测试错误率一栏，就会发现测试错误率在达到了一个最小值之后又开始上升了，我的代码运行出来也是如此。这类现象称之为过拟合。有文献声称，对于表现好的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会随着分类器的增多而上升。或许在本例子中的数据集也称不上“表现好”。该数据集一开始有30%的缺失值，对于Logistic回归而言，这些缺失值的假设就是有效的，而对于决策树却可能并不合适。

所以我又在UCI上找了一个适合二分类的数据集：Connectionist Bench (Sonar, Mines vs. Rocks) Data Set，这是Gorman和Sejnowski在使用神经网络对声纳信号进行分类研究中使用的数据集[1]。任务是训练一个网络，以区分由金属圆柱体弹起的声纳信号和由大致圆柱形岩石弹起的声纳信号。周志华老师在NeC4.5: Neural Ensemble Based C4.5这篇论文中也用到了这个数据集。Adaboost运行的结果如表7-3所示。
**表7-3 Java实现Adaboost在UCI数据集上的结果**
|分类器个数|训练集误差|测试集误差|
|----|----|---|
|1|0.24|0.25|
|10|0.08|0.22|
|50|0.00|0.16|
|100|0.00|0.13|
|500|0.00|0.16|
|1000|0.00|0.14|
|2000|0.00|0.16|
观察表7-3中的测试错误率一栏，AdaBoost的测试错误率逐渐达到一个稳定值，并不会随着分类器的增多而上升，最终稳定在0.16左右。我用之前实现的NB模型跑了一遍，测试集误差为0.35，所以在这个数据集上Adaboost的效果要比NB好一些。

